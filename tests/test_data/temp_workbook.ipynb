{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af6fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from utils.api_utils import AuthenticateSpotify\n",
    "from etl.extract.extract import extract_data\n",
    "from utils.transform_utils import (\n",
    "    get_track_data,\n",
    "    get_tracks_data,\n",
    "    check_in_list,\n",
    "    genre_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b75961",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('test_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cac662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c93a89",
   "metadata": {},
   "source": [
    "### format_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f474351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove spaces and change to lower case\n",
    "tracks.columns = tracks.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# remove special characters\n",
    "tracks.columns = tracks.columns.str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "# rename columns\n",
    "tracks = tracks.rename(columns={'track_uri': 'track_id',\n",
    "                                'artist_uris': 'artist_id',\n",
    "                                'album_uri': 'album_id',\n",
    "                                'album_artist_uris': 'album_artist_id',\n",
    "                                'album_release_date': 'album_year'})\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"format_column_names_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807e52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove user API data\n",
    "tracks = tracks.drop(columns=['added_at', 'added_by'])\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tracks = tracks.drop(columns=['track_preview_url', 'album_genres',\n",
    "                                'copyrights', 'label'])\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"drop_columns_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8143fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "tracks = tracks.drop_duplicates()\n",
    "\n",
    "# standardise date format\n",
    "tracks['album_year'] = pd.to_datetime(tracks['album_year'],\n",
    "                                        errors='coerce').dt.year\n",
    "\n",
    "# drop rows with invalid dates\n",
    "tracks = tracks.dropna(subset=['album_year'])\n",
    "\n",
    "tracks = tracks.dropna(subset=['popularity'])\n",
    "\n",
    "# Convert the year to an integer\n",
    "tracks['album_year'] = tracks['album_year'].astype(int)\n",
    "\n",
    "tracks['explicit'] = tracks['explicit'].map({'True': True, 'False': False})\n",
    "with pd.option_context(\"future.no_silent_downcasting\", True):\n",
    "    tracks['explicit'] = tracks['explicit'].fillna(False).astype(bool)\n",
    "\n",
    "tracks['popularity'] = tracks['popularity'].astype(int)\n",
    "\n",
    "tracks['disc_number'] = tracks['disc_number'].astype(int)\n",
    "\n",
    "tracks['track_number'] = tracks['track_number'].astype(int)\n",
    "\n",
    "tracks['track_duration_ms'] = tracks['track_duration_ms'].astype(int)\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"clean_tracks_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c60bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spotify URIs to IDs\n",
    "tracks['track_id'] = tracks['track_id'].str.replace('spotify:track:',\n",
    "                                                    '', regex=False)\n",
    "\n",
    "tracks['album_id'] = tracks['album_id'].str.replace('spotify:album:', '',\n",
    "                                                    regex=False)\n",
    "tracks['album_artist_id'] = tracks['album_artist_id'].str.replace(\n",
    "                                    'spotify:artist:', '', regex=False\n",
    "                                                                    )\n",
    "# format artist_id\n",
    "tracks['artist_id'] = tracks['artist_id'].str.replace('spotify:artist:',\n",
    "                                                        '', regex=False)\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"convert_uris_to_ids_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f178297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "tracks = tracks.dropna(subset=['track_id', 'track_name',\n",
    "                                'artist_id', 'artist_names',\n",
    "                                'album_year', 'album_id',\n",
    "                                'album_name', 'album_artist_id',\n",
    "                                'album_artist_names', 'album_image_url'])\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"remove_missing_values_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67fc66f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request was successful\n",
      "Request was successful\n",
      "Request was successful\n",
      "Request was successful\n",
      "Request was successful\n"
     ]
    }
   ],
   "source": [
    "dataframe = tracks.copy()\n",
    "\n",
    "updated_df = dataframe.copy()\n",
    "\n",
    "token = AuthenticateSpotify()\n",
    "# process the DataFrame in batches to optimise API calls\n",
    "batch_size = 50\n",
    "\n",
    "for i in range(0, len(dataframe), batch_size):\n",
    "\n",
    "    # select batch of track_id\n",
    "    end_index = min(i + batch_size, len(dataframe))\n",
    "    tracks_ids = dataframe[i:end_index]['track_id'].tolist()\n",
    "\n",
    "    try:\n",
    "        response = get_tracks_data(token, tracks_ids)\n",
    "\n",
    "        # select popularity and a second id for comformation\n",
    "        temp_isrc = [item.get('external_ids')['isrc'] for\n",
    "                        item in response['tracks']]\n",
    "        temp_pop = [item['popularity'] for item in response['tracks']]\n",
    "\n",
    "        # Create a mapping of ISRC to popularity\n",
    "        isrc_pop_map = dict(zip(temp_isrc, temp_pop))\n",
    "\n",
    "        # Update the popularity column in df2 only if ISRC matches\n",
    "        for idx in dataframe[i:end_index].index:\n",
    "            try:\n",
    "                isrc_value = dataframe.loc[idx, 'isrc']\n",
    "                if isrc_value in isrc_pop_map:\n",
    "                    updated_df.loc[idx, 'popularity'] = isrc_pop_map[\n",
    "                            isrc_value\n",
    "                        ]\n",
    "                else:\n",
    "                    # Set as null if ISRC doesn't match\n",
    "                    updated_df.loc[idx, 'popularity'] = pd.NA\n",
    "            except Exception as row_error:\n",
    "\n",
    "                print(f\"Error processing row {idx}: {row_error}\")\n",
    "                # Set as null for problematic rows\n",
    "                updated_df.loc[idx, 'popularity'] = pd.NA\n",
    "    except Exception as batch_error:\n",
    "        # if a batch fails, move to row by row updation for that batch\n",
    "\n",
    "        for j in range(i, end_index):\n",
    "            try:\n",
    "                # Attempt to process each track IDs individually\n",
    "                track_id = dataframe.loc[j, 'track_id']\n",
    "                a = get_track_data(token, track_id)\n",
    "\n",
    "                if 'isrc' in a.get('external_ids'):\n",
    "                    isrc_value = a.get('external_ids')['isrc']\n",
    "\n",
    "                    if isrc_value == updated_df.loc[j, 'isrc']:\n",
    "                        updated_df.loc[j, 'popularity'] = a['popularity']\n",
    "                else:\n",
    "                    updated_df.loc[j, 'popularity'] = None\n",
    "                    print(\n",
    "                            \"ISRC not found for track ID {j}\".format(\n",
    "                            j=j\n",
    "                            )\n",
    "                        )\n",
    "                    print(\n",
    "                            \"IRSC: {irsc}\".format(\n",
    "                                irsc=updated_df.loc[j, 'isrc']\n",
    "                                )\n",
    "                            )\n",
    "            except Exception as e:\n",
    "                # if the row updation fail print error and continue\n",
    "                print(\n",
    "                        \"Error processing track ID {j}, irsc {irsc}\".format(\n",
    "                        j=j, irsc=updated_df.loc[j, 'isrc']\n",
    "                        )\n",
    "                    )\n",
    "                print(\n",
    "                    \"Error: {e}\".format(e=e)\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        # prin tth ebatch that failed and the IRSC ids in it\n",
    "        print(f\"Error processing batch {i}-{end_index}: {batch_error}\")\n",
    "        print(f\"Track IDs in batch: {tracks_ids}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "updated_df.reset_index(inplace=True)\n",
    "updated_df = updated_df.dropna(subset=['popularity'])\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"update_API_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e9b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 8578 rows from c:\\users\\ashle\\documents\\github\\df_capstone\\etl\\extract\\../../data/clean/transformed_data.csv in 0.0580 seconds\n"
     ]
    }
   ],
   "source": [
    "filepath = '../../data/clean/transformed_data.csv'\n",
    "from etl.extract.extract import extract_data\n",
    "new_pop_data = extract_data(file=filepath)\n",
    "\n",
    "# Create a mapping of ISRC to popularity\n",
    "new_pop_data_map = dict(zip(new_pop_data['track_id'],\n",
    "                            new_pop_data['popularity']))\n",
    "\n",
    "tracks.set_index('track_id', inplace=True)\n",
    "\n",
    "for idx in tracks.index:\n",
    "    tracks.loc[idx, 'popularity'] = new_pop_data_map.get(idx, pd.NA)\n",
    "\n",
    "# Reset index to avoid side effects\n",
    "tracks.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "tracks = tracks.dropna(subset=['popularity'])\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"update_data_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ed161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transform_utils import genre_dict, check_in_list\n",
    "\n",
    "# import genres dictionary\n",
    "genres = genre_dict()\n",
    "\n",
    "# add genre count column for missing value count\n",
    "for key, value in genres.items():\n",
    "    tracks[key] = tracks.apply(lambda x: check_in_list(x['artist_genres'],\n",
    "                                                        value), axis=1)\n",
    "\n",
    "tracks['genre_count'] = tracks[genres.keys()].sum(axis=1)\n",
    "tracks = tracks.drop(columns=['artist_genres'])\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"simplify_and_expand_artist_genres_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82a2f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf327b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Track URI  \\\n",
      "0  spotify:track:1XAZlnVtthcDZt2NI1Dtxo   \n",
      "1  spotify:track:6a8GbQIlV8HBUW3c6Uk9PH   \n",
      "2  spotify:track:70XtWbcVZcpaOddJftMcVi   \n",
      "3  spotify:track:1NXUWyPJk5kO6DQJ5t7bDu   \n",
      "4  spotify:track:72WZtWs6V7uu3aMgMmEkYe   \n",
      "\n",
      "                                Track Name  \\\n",
      "0  Justified & Ancient - Stand by the Jams   \n",
      "1          I Know You Want Me (Calle Ocho)   \n",
      "2       From the Bottom of My Broken Heart   \n",
      "3         Apeman - 2014 Remastered Version   \n",
      "4       You Can't Always Get What You Want   \n",
      "\n",
      "                           Artist URI(s)      Artist Name(s)  \\\n",
      "0  spotify:artist:6dYrdRlNZSKaVxYg5IrvCH             The KLF   \n",
      "1  spotify:artist:0TnOYISbd1XYRBk9myaseg             Pitbull   \n",
      "2  spotify:artist:26dSoYclwsYLMAKD3tpOr4      Britney Spears   \n",
      "3  spotify:artist:1SQRv42e4PjEYfPhS0Tk9E           The Kinks   \n",
      "4  spotify:artist:22bE4uQ6baNwSHPVcDxLCe  The Rolling Stones   \n",
      "\n",
      "                              Album URI  \\\n",
      "0  spotify:album:4MC0ZjNtVP1nDD5lsLxFjc   \n",
      "1  spotify:album:5xLAcbvbSAlRtPXnKkggXA   \n",
      "2  spotify:album:3WNxdumkSMGMJRhEgK80qx   \n",
      "3  spotify:album:6lL6HugNEN4Vlc8sj0Zcse   \n",
      "4  spotify:album:0c78nsgqX6VfniSNWIxwoD   \n",
      "\n",
      "                                          Album Name  \\\n",
      "0                                   Songs Collection   \n",
      "1                     Pitbull Starring In Rebelution   \n",
      "2     ...Baby One More Time (Digital Deluxe Version)   \n",
      "3  Lola vs. Powerman and the Moneygoround, Pt. On...   \n",
      "4                                       Let It Bleed   \n",
      "\n",
      "                     Album Artist URI(s) Album Artist Name(s)  \\\n",
      "0  spotify:artist:6dYrdRlNZSKaVxYg5IrvCH              The KLF   \n",
      "1  spotify:artist:0TnOYISbd1XYRBk9myaseg              Pitbull   \n",
      "2  spotify:artist:26dSoYclwsYLMAKD3tpOr4       Britney Spears   \n",
      "3  spotify:artist:1SQRv42e4PjEYfPhS0Tk9E            The Kinks   \n",
      "4  spotify:artist:22bE4uQ6baNwSHPVcDxLCe   The Rolling Stones   \n",
      "\n",
      "  Album Release Date                                    Album Image URL  ...  \\\n",
      "0         1992-08-03  https://i.scdn.co/image/ab67616d0000b27355346b...  ...   \n",
      "1         2009-10-23  https://i.scdn.co/image/ab67616d0000b27326d73a...  ...   \n",
      "2         1999-01-12  https://i.scdn.co/image/ab67616d0000b2738e4986...  ...   \n",
      "3         2014-10-20  https://i.scdn.co/image/ab67616d0000b2731e7c53...  ...   \n",
      "4         1969-12-05  https://i.scdn.co/image/ab67616d0000b27373d927...  ...   \n",
      "\n",
      "   Speechiness  Acousticness  Instrumentalness Liveness  Valence    Tempo  \\\n",
      "0       0.0480        0.0158          0.112000   0.4080    0.504  111.458   \n",
      "1       0.1490        0.0142          0.000021   0.2370    0.800  127.045   \n",
      "2       0.0305        0.5600          0.000001   0.3380    0.706   74.981   \n",
      "3       0.2590        0.5680          0.000051   0.0384    0.833   75.311   \n",
      "4       0.0687        0.6750          0.000073   0.2890    0.497   85.818   \n",
      "\n",
      "  Time Signature Album Genres                                Label  \\\n",
      "0              4          NaN                  Jams Communications   \n",
      "1              4          NaN  Mr.305/Polo Grounds Music/J Records   \n",
      "2              4          NaN                                 Jive   \n",
      "3              4          NaN                    Sanctuary Records   \n",
      "4              4          NaN                Universal Music Group   \n",
      "\n",
      "                                          Copyrights  \n",
      "0  C 1992 Copyright Control, P 1992 Jams Communic...  \n",
      "1  P (P) 2009 RCA/JIVE Label Group, a unit of Son...  \n",
      "2                     P (P) 1999 Zomba Recording LLC  \n",
      "3  C © 2014 Sanctuary Records Group Ltd., a BMG C...  \n",
      "4  C © 2002 ABKCO Music & Records Inc., P ℗ 2002 ...  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "Extracted 8578 rows from c:\\users\\ashle\\documents\\github\\df_capstone\\etl\\extract\\../../data/clean/transformed_data.csv in 0.0489 seconds\n",
      "                 track_id                               track_name  \\\n",
      "0  1XAZlnVtthcDZt2NI1Dtxo  Justified & Ancient - Stand by the Jams   \n",
      "1  6a8GbQIlV8HBUW3c6Uk9PH          I Know You Want Me (Calle Ocho)   \n",
      "2  70XtWbcVZcpaOddJftMcVi       From the Bottom of My Broken Heart   \n",
      "3  1NXUWyPJk5kO6DQJ5t7bDu         Apeman - 2014 Remastered Version   \n",
      "4  72WZtWs6V7uu3aMgMmEkYe       You Can't Always Get What You Want   \n",
      "\n",
      "                artist_id        artist_names                album_id  \\\n",
      "0  6dYrdRlNZSKaVxYg5IrvCH             The KLF  4MC0ZjNtVP1nDD5lsLxFjc   \n",
      "1  0TnOYISbd1XYRBk9myaseg             Pitbull  5xLAcbvbSAlRtPXnKkggXA   \n",
      "2  26dSoYclwsYLMAKD3tpOr4      Britney Spears  3WNxdumkSMGMJRhEgK80qx   \n",
      "3  1SQRv42e4PjEYfPhS0Tk9E           The Kinks  6lL6HugNEN4Vlc8sj0Zcse   \n",
      "4  22bE4uQ6baNwSHPVcDxLCe  The Rolling Stones  0c78nsgqX6VfniSNWIxwoD   \n",
      "\n",
      "                                          album_name         album_artist_id  \\\n",
      "0                                   Songs Collection  6dYrdRlNZSKaVxYg5IrvCH   \n",
      "1                     Pitbull Starring In Rebelution  0TnOYISbd1XYRBk9myaseg   \n",
      "2     ...Baby One More Time (Digital Deluxe Version)  26dSoYclwsYLMAKD3tpOr4   \n",
      "3  Lola vs. Powerman and the Moneygoround, Pt. On...  1SQRv42e4PjEYfPhS0Tk9E   \n",
      "4                                       Let It Bleed  22bE4uQ6baNwSHPVcDxLCe   \n",
      "\n",
      "   album_artist_names  album_year  \\\n",
      "0             The KLF        1992   \n",
      "1             Pitbull        2009   \n",
      "2      Britney Spears        1999   \n",
      "3           The Kinks        2014   \n",
      "4  The Rolling Stones        1969   \n",
      "\n",
      "                                     album_image_url  ...  country    ska  \\\n",
      "0  https://i.scdn.co/image/ab67616d0000b27355346b...  ...    False  False   \n",
      "1  https://i.scdn.co/image/ab67616d0000b27326d73a...  ...    False  False   \n",
      "2  https://i.scdn.co/image/ab67616d0000b2738e4986...  ...    False  False   \n",
      "3  https://i.scdn.co/image/ab67616d0000b2731e7c53...  ...    False  False   \n",
      "4  https://i.scdn.co/image/ab67616d0000b27373d927...  ...    False  False   \n",
      "\n",
      "   dance_disco  indie_alt  retro_vintage novelty  easy_listening  cultural  \\\n",
      "0        False      False          False   False           False     False   \n",
      "1         True      False          False   False           False     False   \n",
      "2         True      False          False   False           False     False   \n",
      "3        False       True           True   False           False      True   \n",
      "4        False      False           True   False           False      True   \n",
      "\n",
      "    jazz  genre_count  \n",
      "0  False            1  \n",
      "1  False            3  \n",
      "2  False            2  \n",
      "3  False            5  \n",
      "4  False            3  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "from etl.transform.transform import transform_data\n",
    "tracks = pd.read_csv('test_input.csv')\n",
    "print(tracks.head())\n",
    "try:\n",
    "    tracks = transform_data(tracks, state=\"old\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"test_output_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8d4e002",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['added_at', 'added_by'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tracks = \u001b[43mtransform_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mold\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# save the transformed data tpo csv to test against sql database\u001b[39;00m\n\u001b[32m      4\u001b[39m filepath = os.path.join(\u001b[33m\"\u001b[39m\u001b[33mtest_output_data.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\users\\ashle\\documents\\github\\df_capstone\\etl\\transform\\transform.py:21\u001b[39m, in \u001b[36mtransform_data\u001b[39m\u001b[34m(tracks, state)\u001b[39m\n\u001b[32m     18\u001b[39m tracks = format_column_names(tracks)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# remove user API data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m tracks = \u001b[43mdrop_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# changes data format to make it compatible with API\u001b[39;00m\n\u001b[32m     24\u001b[39m tracks = convert_uris_to_ids(tracks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\users\\ashle\\documents\\github\\df_capstone\\etl\\transform\\transform.py:241\u001b[39m, in \u001b[36mdrop_columns\u001b[39m\u001b[34m(tracks)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[33;03mRemove columns\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# remove user API data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m tracks = \u001b[43mtracks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madded_at\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madded_by\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# remove unnecessary columns\u001b[39;00m\n\u001b[32m    244\u001b[39m tracks = tracks.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mtrack_preview_url\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33malbum_genres\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    245\u001b[39m                               \u001b[33m'\u001b[39m\u001b[33mcopyrights\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashle\\Documents\\GitHub\\DF_capstone\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5435\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5443\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5579\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5583\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5587\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5588\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashle\\Documents\\GitHub\\DF_capstone\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4786\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4791\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashle\\Documents\\GitHub\\DF_capstone\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4828\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4830\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4831\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4833\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4834\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashle\\Documents\\GitHub\\DF_capstone\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7071\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['added_at', 'added_by'] not found in axis\""
     ]
    }
   ],
   "source": [
    "tracks = transform_data(tracks, state=\"old\")\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"test_output_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "636557df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"test_output_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = tracks['explicit'].astype(bool)\n",
    "tracks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
