{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6af6fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from utils.api_utils import AuthenticateSpotify\n",
    "from etl.extract.extract import extract_data\n",
    "from utils.transform_utils import (\n",
    "    get_track_data,\n",
    "    get_tracks_data,\n",
    "    check_in_list,\n",
    "    genre_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43b75961",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('test_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13cac662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c93a89",
   "metadata": {},
   "source": [
    "### format_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f474351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove spaces and change to lower case\n",
    "tracks.columns = tracks.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# remove special characters\n",
    "tracks.columns = tracks.columns.str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "# rename columns\n",
    "tracks = tracks.rename(columns={'track_uri': 'track_id',\n",
    "                                'artist_uris': 'artist_id',\n",
    "                                'album_uri': 'album_id',\n",
    "                                'album_artist_uris': 'album_artist_id',\n",
    "                                'album_release_date': 'album_year'})\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"format_column_names_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "807e52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove user API data\n",
    "tracks = tracks.drop(columns=['added_at', 'added_by'])\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tracks = tracks.drop(columns=['track_preview_url', 'album_genres',\n",
    "                                'copyrights', 'label'])\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"drop_columns_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea8143fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "tracks = tracks.drop_duplicates()\n",
    "\n",
    "# standardise date format\n",
    "tracks['album_year'] = pd.to_datetime(tracks['album_year'],\n",
    "                                        errors='coerce').dt.year\n",
    "\n",
    "# drop rows with invalid dates\n",
    "tracks = tracks.dropna(subset=['album_year'])\n",
    "\n",
    "# Convert the year to an integer\n",
    "tracks['album_year'] = tracks['album_year'].astype(int)\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"clean_tracks_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66c60bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spotify URIs to IDs\n",
    "tracks['track_id'] = tracks['track_id'].str.replace('spotify:track:',\n",
    "                                                    '', regex=False)\n",
    "\n",
    "tracks['album_id'] = tracks['album_id'].str.replace('spotify:album:', '',\n",
    "                                                    regex=False)\n",
    "tracks['album_artist_id'] = tracks['album_artist_id'].str.replace(\n",
    "                                    'spotify:artist:', '', regex=False\n",
    "                                                                    )\n",
    "# format artist_id\n",
    "tracks['artist_id'] = tracks['artist_id'].str.replace('spotify:artist:',\n",
    "                                                        '', regex=False)\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"convert_uris_to_ids_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f178297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "tracks = tracks.dropna(subset=['track_id', 'track_name',\n",
    "                                'artist_id', 'artist_names',\n",
    "                                'album_year', 'album_id',\n",
    "                                'album_name', 'album_artist_id',\n",
    "                                'album_artist_names', 'album_image_url'])\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"remove_missing_values_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67fc66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = tracks.copy()\n",
    "\n",
    "updated_df = dataframe.copy()\n",
    "\n",
    "token = AuthenticateSpotify()\n",
    "# process the DataFrame in batches to optimise API calls\n",
    "batch_size = 50\n",
    "\n",
    "for i in range(0, len(dataframe), batch_size):\n",
    "\n",
    "    # select batch of track_id\n",
    "    end_index = min(i + batch_size, len(dataframe))\n",
    "    tracks_ids = dataframe[i:end_index]['track_id'].tolist()\n",
    "\n",
    "    try:\n",
    "        response = get_tracks_data(token, tracks_ids)\n",
    "\n",
    "        # select popularity and a second id for comformation\n",
    "        temp_isrc = [item.get('external_ids')['isrc'] for\n",
    "                        item in response['tracks']]\n",
    "        temp_pop = [item['popularity'] for item in response['tracks']]\n",
    "\n",
    "        # Create a mapping of ISRC to popularity\n",
    "        isrc_pop_map = dict(zip(temp_isrc, temp_pop))\n",
    "\n",
    "        # Update the popularity column in df2 only if ISRC matches\n",
    "        for idx in dataframe[i:end_index].index:\n",
    "            try:\n",
    "                isrc_value = dataframe.loc[idx, 'isrc']\n",
    "                if isrc_value in isrc_pop_map:\n",
    "                    updated_df.loc[idx, 'popularity'] = isrc_pop_map[\n",
    "                            isrc_value\n",
    "                        ]\n",
    "                else:\n",
    "                    # Set as null if ISRC doesn't match\n",
    "                    updated_df.loc[idx, 'popularity'] = pd.NA\n",
    "            except Exception as row_error:\n",
    "\n",
    "                print(f\"Error processing row {idx}: {row_error}\")\n",
    "                # Set as null for problematic rows\n",
    "                updated_df.loc[idx, 'popularity'] = pd.NA\n",
    "    except Exception as batch_error:\n",
    "        # if a batch fails, move to row by row updation for that batch\n",
    "\n",
    "        for j in range(i, end_index):\n",
    "            try:\n",
    "                # Attempt to process each track IDs individually\n",
    "                track_id = dataframe.loc[j, 'track_id']\n",
    "                a = get_track_data(token, track_id)\n",
    "\n",
    "                if 'isrc' in a.get('external_ids'):\n",
    "                    isrc_value = a.get('external_ids')['isrc']\n",
    "\n",
    "                    if isrc_value == updated_df.loc[j, 'isrc']:\n",
    "                        updated_df.loc[j, 'popularity'] = a['popularity']\n",
    "                else:\n",
    "                    updated_df.loc[j, 'popularity'] = None\n",
    "                    print(\n",
    "                            \"ISRC not found for track ID {j}\".format(\n",
    "                            j=j\n",
    "                            )\n",
    "                        )\n",
    "                    print(\n",
    "                            \"IRSC: {irsc}\".format(\n",
    "                                irsc=updated_df.loc[j, 'isrc']\n",
    "                                )\n",
    "                            )\n",
    "            except Exception as e:\n",
    "                # if the row updation fail print error and continue\n",
    "                print(\n",
    "                        \"Error processing track ID {j}, irsc {irsc}\".format(\n",
    "                        j=j, irsc=updated_df.loc[j, 'isrc']\n",
    "                        )\n",
    "                    )\n",
    "                print(\n",
    "                    \"Error: {e}\".format(e=e)\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        # prin tth ebatch that failed and the IRSC ids in it\n",
    "        print(f\"Error processing batch {i}-{end_index}: {batch_error}\")\n",
    "        print(f\"Track IDs in batch: {tracks_ids}\")\n",
    "        continue\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"update_API_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57e9b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 8578 rows from c:\\users\\ashle\\documents\\github\\df_capstone\\etl\\extract\\../../data/clean/transformed_data.csv in 0.0442 seconds\n"
     ]
    }
   ],
   "source": [
    "filepath = '../../data/clean/transformed_data.csv'\n",
    "from etl.extract.extract import extract_data\n",
    "new_pop_data = extract_data(file=filepath)\n",
    "\n",
    "# Create a mapping of ISRC to popularity\n",
    "new_pop_data_map = dict(zip(new_pop_data['track_id'],\n",
    "                            new_pop_data['popularity']))\n",
    "\n",
    "tracks.set_index('track_id')\n",
    "for idx in tracks:\n",
    "    if idx in new_pop_data_map:\n",
    "        tracks.loc[idx, 'popularity'] = new_pop_data_map[\n",
    "                    idx\n",
    "                ]\n",
    "    else:\n",
    "        tracks.loc[idx, 'popularity'] = pd.NA\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"update_data_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3ed161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transform_utils import genre_dict, check_in_list\n",
    "\n",
    "# import genres dictionary\n",
    "genres = genre_dict()\n",
    "\n",
    "# add genre count column for missing value count\n",
    "for key, value in genres.items():\n",
    "    tracks[key] = tracks.apply(lambda x: check_in_list(x['artist_genres'],\n",
    "                                                        value), axis=1)\n",
    "\n",
    "tracks['genre_count'] = tracks[genres.keys()].sum(axis=1)\n",
    "tracks = tracks.drop(columns=['artist_genres'])\n",
    "\n",
    "# save the transformed data tpo csv to test against sql database\n",
    "filepath = os.path.join(\"simplify_and_expand_artist_genres_data.csv\")\n",
    "tracks.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
